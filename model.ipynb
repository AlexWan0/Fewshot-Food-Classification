{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "#food_dataset = load_dataset('food101')\n",
    "food_dataset = load_from_disk('dataset/salads/hf_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#with open('mapping.json') as f_in:\n",
    "with open('dataset/salads/mapping_salad.json') as f_in:\n",
    "    name_to_index = json.load(f_in)\n",
    "\n",
    "index_to_name = {v: k for k, v in name_to_index.items()}\n",
    "\n",
    "num_classes = len(index_to_name)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "label_index = {}\n",
    "\n",
    "for i, lbl in enumerate(food_dataset['train']['label']):\n",
    "    if lbl not in label_index:\n",
    "        label_index[lbl] = []\n",
    "    \n",
    "    # if len(label_index[lbl]) >= hash(lbl) % 10 + 1:\n",
    "        # continue\n",
    "    \n",
    "    label_index[lbl].append(i)\n",
    "\n",
    "fs_indices = list(chain(*label_index.values()))\n",
    "fs_subset = food_dataset['train'].select(fs_indices)\n",
    "\n",
    "print(len(fs_subset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from itertools import islice\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def label_to_string(label, template='A photo of a {name}. A picture of food.'):\n",
    "    label_name = index_to_name[label]\n",
    "    label_name = label_name.replace('_', ' ')\n",
    "    return template.format(name=label_name)\n",
    "\n",
    "def article_text_shorten(text, num_sentences=2):\n",
    "    sents = islice(nlp(text).sents, num_sentences)\n",
    "    sents = [str(s) for s in sents]\n",
    "    return ' '.join(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "hidden_size = 768\n",
    "\n",
    "# laion/CLIP-ViT-H-14-laion2B-s32B-b79K\n",
    "# laion/CLIP-ViT-B-32-laion2B-s34B-b79K\n",
    "# openai/clip-vit-large-patch14\n",
    "def load_clip_model(device='cuda', model_name=\"openai/clip-vit-large-patch14\"):\n",
    "    model = CLIPModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "model, processor = load_clip_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 256/588 [00:44<00:57,  5.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m label_str \u001b[39m=\u001b[39m label_to_string(label)\n\u001b[1;32m     14\u001b[0m \u001b[39m#label_str = article_text_shorten(ex['text'], num_sentences=1)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model_input \u001b[39m=\u001b[39m processor(\n\u001b[1;32m     17\u001b[0m     images\u001b[39m=\u001b[39;49mimage, text\u001b[39m=\u001b[39;49mlabel_str, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m model_output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_input)\n\u001b[1;32m     22\u001b[0m ref_images[i] \u001b[39m=\u001b[39m model_output\u001b[39m.\u001b[39mimage_embeds\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/models/clip/processing_clip.py:102\u001b[0m, in \u001b[0;36mCLIPProcessor.__call__\u001b[0;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(text, return_tensors\u001b[39m=\u001b[39mreturn_tensors, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m images \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_processor(images, return_tensors\u001b[39m=\u001b[39;49mreturn_tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m images \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     encoding[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m image_features\u001b[39m.\u001b[39mpixel_values\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/image_processing_utils.py:446\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m    445\u001b[0m     \u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/models/clip/image_processing_clip.py:318\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m images \u001b[39m=\u001b[39m [to_numpy_array(image) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 318\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize(image\u001b[39m=\u001b[39mimage, size\u001b[39m=\u001b[39msize, resample\u001b[39m=\u001b[39mresample) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m do_center_crop:\n\u001b[1;32m    321\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcenter_crop(image\u001b[39m=\u001b[39mimage, size\u001b[39m=\u001b[39mcrop_size) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/models/clip/image_processing_clip.py:318\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    315\u001b[0m images \u001b[39m=\u001b[39m [to_numpy_array(image) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 318\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresize(image\u001b[39m=\u001b[39;49mimage, size\u001b[39m=\u001b[39;49msize, resample\u001b[39m=\u001b[39;49mresample) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m do_center_crop:\n\u001b[1;32m    321\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcenter_crop(image\u001b[39m=\u001b[39mimage, size\u001b[39m=\u001b[39mcrop_size) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/models/clip/image_processing_clip.py:144\u001b[0m, in \u001b[0;36mCLIPImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe `size` parameter must contain the key `shortest_edge`. Got \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m output_size \u001b[39m=\u001b[39m get_resize_output_image_size(image, size\u001b[39m=\u001b[39msize[\u001b[39m\"\u001b[39m\u001b[39mshortest_edge\u001b[39m\u001b[39m\"\u001b[39m], default_to_square\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 144\u001b[0m \u001b[39mreturn\u001b[39;00m resize(image, size\u001b[39m=\u001b[39;49moutput_size, resample\u001b[39m=\u001b[39;49mresample, data_format\u001b[39m=\u001b[39;49mdata_format, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/image_transforms.py:278\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[1;32m    276\u001b[0m height, width \u001b[39m=\u001b[39m size\n\u001b[1;32m    277\u001b[0m \u001b[39m# PIL images are in the format (width, height)\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m resized_image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39;49mresize((width, height), resample\u001b[39m=\u001b[39;49mresample, reducing_gap\u001b[39m=\u001b[39;49mreducing_gap)\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m return_numpy:\n\u001b[1;32m    281\u001b[0m     resized_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(resized_image)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/PIL/Image.py:2192\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2184\u001b[0m             \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39m, factor, box\u001b[39m=\u001b[39mreduce_box)\n\u001b[1;32m   2185\u001b[0m         box \u001b[39m=\u001b[39m (\n\u001b[1;32m   2186\u001b[0m             (box[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2187\u001b[0m             (box[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2188\u001b[0m             (box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2189\u001b[0m             (box[\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2190\u001b[0m         )\n\u001b[0;32m-> 2192\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mresize(size, resample, box))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "ref_images = torch.zeros((len(fs_subset), hidden_size))\n",
    "ref_text = torch.zeros((num_classes, hidden_size))\n",
    "labels_all = []\n",
    "\n",
    "for i, ex in enumerate(tqdm(fs_subset)):\n",
    "    image, label = ex['image'], ex['label']\n",
    "    labels_all.append(label)\n",
    "    \n",
    "    label_str = label_to_string(label)\n",
    "    #label_str = article_text_shorten(ex['text'], num_sentences=1)\n",
    "\n",
    "    model_input = processor(\n",
    "        images=image, text=label_str, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).to('cuda')\n",
    "\n",
    "    model_output = model(**model_input)\n",
    "\n",
    "    ref_images[i] = model_output.image_embeds.detach().cpu()\n",
    "    ref_text[label] = model_output.text_embeds.detach().cpu()\n",
    "\n",
    "labels_oh = F.one_hot(torch.tensor(labels_all), num_classes=num_classes).float()\n",
    "labels_oh = labels_oh / torch.sum(labels_oh, dim=0, keepdim=True)\n",
    "assert not torch.isnan(labels_oh).any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main TIP-Adapter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_single(test_image, beta=1.0, alpha=1.0):\n",
    "    test_image_embeds = model(\n",
    "        **(processor(images=test_image, text='', return_tensors=\"pt\", padding=True).to('cuda'))\n",
    "    ).image_embeds.cpu()\n",
    "\n",
    "    img_sim = torch.matmul(test_image_embeds, ref_images.T)\n",
    "    img_sim = ((-1) * (beta - beta * img_sim)).exp()\n",
    "    class_sim_img = torch.matmul(img_sim, labels_oh) # (1, num_classes)\n",
    "\n",
    "    class_sim_text = torch.matmul(test_image_embeds, ref_text.T) # (1, num_classes)\n",
    "\n",
    "    class_sim = alpha * class_sim_img + class_sim_text # (1, num_classes)\n",
    "\n",
    "    return class_sim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "food_dataset['validation'] = food_dataset['validation'].shuffle()\n",
    "\n",
    "def validate(alpha=1.0, beta=1.0, limit_test=500, verbose=False):\n",
    "    labels = []\n",
    "    preds = []\n",
    "    top_5_all = []\n",
    "\n",
    "    total_test = min(len(food_dataset['validation']), limit_test)\n",
    "\n",
    "    for test_idx, test_sample in enumerate(tqdm(food_dataset['validation'], total=total_test)):\n",
    "        if test_idx >= limit_test:\n",
    "            break\n",
    "        \n",
    "        test_label = test_sample['label']\n",
    "        test_image = test_sample['image']\n",
    "\n",
    "        logits = infer_single(test_image, alpha=alpha, beta=beta)\n",
    "        preds_top_5 = torch.sort(logits, descending=True)[1][0,:5]\n",
    "        preds_top_1 = preds_top_5[0]\n",
    "\n",
    "        labels.append(test_label)\n",
    "        preds.append(preds_top_1)\n",
    "        top_5_all.append(test_label in preds_top_5)\n",
    "\n",
    "    if verbose:\n",
    "        print(classification_report(labels, preds))\n",
    "\n",
    "    return accuracy_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 51/65 [00:09<00:02,  5.67it/s]/home/alexwan/miniconda/lib/python3.9/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 65/65 [00:12<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       1.00      1.00      1.00         1\n",
      "          13       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         1\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.00      0.00      0.00         0\n",
      "          30       0.00      0.00      0.00         0\n",
      "          32       0.00      0.00      0.00         1\n",
      "          36       1.00      1.00      1.00         1\n",
      "          40       0.00      0.00      0.00         0\n",
      "          41       0.00      0.00      0.00         0\n",
      "          42       0.00      0.00      0.00         0\n",
      "          49       0.00      0.00      0.00         0\n",
      "          51       0.00      0.00      0.00         0\n",
      "          53       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         0\n",
      "          56       1.00      1.00      1.00         1\n",
      "          58       0.00      0.00      0.00         0\n",
      "          60       1.00      1.00      1.00         1\n",
      "          61       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         0\n",
      "          66       0.00      0.00      0.00         2\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       0.00      0.00      0.00         1\n",
      "          70       1.00      1.00      1.00         1\n",
      "          74       0.00      0.00      0.00         0\n",
      "          75       1.00      1.00      1.00         1\n",
      "          76       1.00      1.00      1.00         1\n",
      "          77       1.00      1.00      1.00         1\n",
      "          82       1.00      1.00      1.00         1\n",
      "          84       0.00      0.00      0.00         0\n",
      "          85       0.00      0.00      0.00         0\n",
      "          88       0.00      0.00      0.00         0\n",
      "          90       0.00      0.00      0.00         2\n",
      "          93       0.00      0.00      0.00         1\n",
      "          95       0.50      0.33      0.40         3\n",
      "          99       1.00      1.00      1.00         1\n",
      "         102       0.50      0.50      0.50         2\n",
      "         107       0.00      0.00      0.00         0\n",
      "         110       0.00      0.00      0.00         1\n",
      "         111       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         0\n",
      "         114       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         0\n",
      "         117       0.00      0.00      0.00         1\n",
      "         119       0.00      0.00      0.00         2\n",
      "         120       0.00      0.00      0.00         1\n",
      "         122       1.00      0.25      0.40         4\n",
      "         124       0.00      0.00      0.00         2\n",
      "         128       0.00      0.00      0.00         1\n",
      "         129       0.00      0.00      0.00         1\n",
      "         134       0.00      0.00      0.00         1\n",
      "         141       0.00      0.00      0.00         0\n",
      "         143       0.00      0.00      0.00         0\n",
      "         146       0.00      0.00      0.00         1\n",
      "         149       0.00      0.00      0.00         0\n",
      "         150       0.33      0.50      0.40         2\n",
      "         151       0.00      0.00      0.00         1\n",
      "         152       1.00      1.00      1.00         2\n",
      "         155       0.00      0.00      0.00         1\n",
      "         157       0.00      0.00      0.00         1\n",
      "         159       1.00      1.00      1.00         1\n",
      "         161       0.00      0.00      0.00         1\n",
      "         162       0.00      0.00      0.00         0\n",
      "         167       1.00      1.00      1.00         2\n",
      "         170       0.00      0.00      0.00         1\n",
      "         174       0.00      0.00      0.00         0\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         1\n",
      "         179       1.00      1.00      1.00         1\n",
      "         180       0.00      0.00      0.00         0\n",
      "         181       0.00      0.00      0.00         0\n",
      "         182       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.31        65\n",
      "   macro avg       0.20      0.19      0.20        65\n",
      "weighted avg       0.36      0.31      0.32        65\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/alexwan/miniconda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alexwan/miniconda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alexwan/miniconda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alexwan/miniconda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alexwan/miniconda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alexwan/miniconda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3076923076923077"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(alpha=0.1, beta=0.1, limit_test=5000, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing {'alpha': 0.01, 'beta': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_score=0.958 > best_score=-inf; best_config={'alpha': 0.01, 'beta': 0.01}\n",
      "testing {'alpha': 0.01, 'beta': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new =_score=0.958 <= best_score=0.958; best_config={'alpha': 0.01, 'beta': 0.01}\n",
      "testing {'alpha': 0.01, 'beta': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:23<00:00, 21.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new =_score=0.958 <= best_score=0.958; best_config={'alpha': 0.01, 'beta': 0.01}\n",
      "testing {'alpha': 0.01, 'beta': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:23<00:00, 21.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_score=0.96 > best_score=0.958; best_config={'alpha': 0.01, 'beta': 2}\n",
      "testing {'alpha': 0.01, 'beta': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new =_score=0.96 <= best_score=0.96; best_config={'alpha': 0.01, 'beta': 2}\n",
      "testing {'alpha': 0.1, 'beta': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new =_score=0.958 <= best_score=0.96; best_config={'alpha': 0.01, 'beta': 2}\n",
      "testing {'alpha': 0.1, 'beta': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new =_score=0.958 <= best_score=0.96; best_config={'alpha': 0.01, 'beta': 2}\n",
      "testing {'alpha': 0.1, 'beta': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new =_score=0.96 <= best_score=0.96; best_config={'alpha': 0.01, 'beta': 2}\n",
      "testing {'alpha': 0.1, 'beta': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_score=0.962 > best_score=0.96; best_config={'alpha': 0.1, 'beta': 2}\n",
      "testing {'alpha': 0.1, 'beta': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new =_score=0.962 <= best_score=0.962; best_config={'alpha': 0.1, 'beta': 2}\n",
      "testing {'alpha': 1, 'beta': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 365/500 [00:17<00:06, 20.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m hyps \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(param_grid\u001b[39m.\u001b[39mkeys(), combination))\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtesting \u001b[39m\u001b[39m{\u001b[39;00mhyps\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m score \u001b[39m=\u001b[39m validate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhyps)\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m score \u001b[39m>\u001b[39m best_score:\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnew_score=\u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m > best_score=\u001b[39m\u001b[39m{\u001b[39;00mbest_score\u001b[39m}\u001b[39;00m\u001b[39m; best_config=\u001b[39m\u001b[39m{\u001b[39;00mhyps\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[93], line 17\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(alpha, beta, limit_test, verbose)\u001b[0m\n\u001b[1;32m     14\u001b[0m test_label \u001b[39m=\u001b[39m test_sample[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m test_image \u001b[39m=\u001b[39m test_sample[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m logits \u001b[39m=\u001b[39m infer_single(test_image, alpha\u001b[39m=\u001b[39;49malpha, beta\u001b[39m=\u001b[39;49mbeta)\n\u001b[1;32m     18\u001b[0m preds_top_5 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msort(logits, descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m,:\u001b[39m5\u001b[39m]\n\u001b[1;32m     19\u001b[0m preds_top_1 \u001b[39m=\u001b[39m preds_top_5[\u001b[39m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[87], line 2\u001b[0m, in \u001b[0;36minfer_single\u001b[0;34m(test_image, beta, alpha)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfer_single\u001b[39m(test_image, beta\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     test_image_embeds \u001b[39m=\u001b[39m model(\n\u001b[1;32m      3\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(processor(images\u001b[39m=\u001b[39;49mtest_image, text\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m     )\u001b[39m.\u001b[39mimage_embeds\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m      6\u001b[0m     img_sim \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(test_image_embeds, ref_images\u001b[39m.\u001b[39mT)\n\u001b[1;32m      7\u001b[0m     img_sim \u001b[39m=\u001b[39m ((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (beta \u001b[39m-\u001b[39m beta \u001b[39m*\u001b[39m img_sim))\u001b[39m.\u001b[39mexp()\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:1125\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1116\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1118\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_model(\n\u001b[1;32m   1119\u001b[0m     pixel_values\u001b[39m=\u001b[39mpixel_values,\n\u001b[1;32m   1120\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1121\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1122\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1123\u001b[0m )\n\u001b[0;32m-> 1125\u001b[0m text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_model(\n\u001b[1;32m   1126\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1127\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1128\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1129\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1130\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1131\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1132\u001b[0m )\n\u001b[1;32m   1134\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1135\u001b[0m image_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisual_projection(image_embeds)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:725\u001b[0m, in \u001b[0;36mCLIPTextTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     \u001b[39m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     attention_mask \u001b[39m=\u001b[39m _expand_mask(attention_mask, hidden_states\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 725\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    726\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    727\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    728\u001b[0m     causal_attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[1;32m    729\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    730\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    731\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    732\u001b[0m )\n\u001b[1;32m    734\u001b[0m last_hidden_state \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    735\u001b[0m last_hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm(last_hidden_state)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:654\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    647\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    648\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[1;32m    649\u001b[0m         hidden_states,\n\u001b[1;32m    650\u001b[0m         attention_mask,\n\u001b[1;32m    651\u001b[0m         causal_attention_mask,\n\u001b[1;32m    652\u001b[0m     )\n\u001b[1;32m    653\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    655\u001b[0m         hidden_states,\n\u001b[1;32m    656\u001b[0m         attention_mask,\n\u001b[1;32m    657\u001b[0m         causal_attention_mask,\n\u001b[1;32m    658\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    659\u001b[0m     )\n\u001b[1;32m    661\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    663\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:383\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    380\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    382\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 383\u001b[0m hidden_states, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    384\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    385\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    386\u001b[0m     causal_attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[1;32m    387\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    388\u001b[0m )\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    391\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:274\u001b[0m, in \u001b[0;36mCLIPAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    272\u001b[0m query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj(hidden_states) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[1;32m    273\u001b[0m key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[0;32m--> 274\u001b[0m value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj(hidden_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[1;32m    276\u001b[0m proj_shape \u001b[39m=\u001b[39m (bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    277\u001b[0m query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(query_states, tgt_len, bsz)\u001b[39m.\u001b[39mview(\u001b[39m*\u001b[39mproj_shape)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1, 2, 5],\n",
    "    'beta': [0.01, 0.1, 1, 2, 5]\n",
    "}\n",
    "\n",
    "best_hyps = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "for combination in itertools.product(*param_grid.values()):\n",
    "    hyps = dict(zip(param_grid.keys(), combination))\n",
    "    print(f'testing {hyps}')\n",
    "    score = validate(**hyps)\n",
    "\n",
    "    if score > best_score:\n",
    "        print(f'new_score={score} > best_score={best_score}; best_config={hyps}')\n",
    "        best_hyps = hyps\n",
    "        best_score = score\n",
    "    else:\n",
    "        print(f'new =_score={score} <= best_score={best_score}; best_config={best_hyps}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
